target_default: 1_wqpdata

include:
  - lib.yml

packages:
  - dataRetrieval
  - dplyr
  - feather
  - scipiper
  - yaml

file_extensions:
  - feather
  - ind

sources:
  - 1_wqpdata/src/wqp_inventory.R
  - 1_wqpdata/src/wqp_pull.R

targets:

  1_wqpdata:
    depends:
      - lib/cfg/gd_config.yml
      - 1_wqpdata/log/tasks_1_wqp_inventory.ind
      - 1_wqpdata/log/tasks_1_wqp.ind

  # -- prepare for data pull --

  # load wqp-specific config info
  wq_dates:
    command: yaml.load_file("1_wqpdata/cfg/wq_dates.yml")
  wqp_states:
    command: yaml.load_file("1_wqpdata/cfg/wqp_states.yml")
  wqp_codes:
    command: yaml.load_file("1_wqpdata/cfg/wqp_codes.yml")
  wqp_state_codes:
    command: get_wqp_state_codes()
  wqp_pull:
    command: yaml.load_file("1_wqpdata/cfg/wqp_pull.yml")

  # prepare destination folders for intermediate and final output.
  # tmp=temporary folder for holding files to only be created on 1 computer.
  # out=folder to hold .ind and data files corresponding to shared cache or everybody's local build.
  # log=folder for the few indicator files that don't correspond to a data file.
  1_wqpdata/tmp/data:
    command: dir.create(target_name, recursive=I(TRUE), showWarnings=I(FALSE))
  1_wqpdata/out/inventory:
    command: dir.create(target_name, recursive=I(TRUE), showWarnings=I(FALSE))
  1_wqpdata/out/data:
    command: dir.create(target_name, recursive=I(TRUE), showWarnings=I(FALSE))
  1_wqpdata/log:
    command: dir.create(target_name, recursive=I(TRUE), showWarnings=I(FALSE))
  wqp_pull_folders:
    command: list(
      tmp='1_wqpdata/tmp/data',
      out='1_wqpdata/out/data',
      log='1_wqpdata/log')

  # -- get inventory of observations available to download --

  # get an inventory of WQP sites and sample counts. for this and all
  # shared-cache targets (and those that depend on any shared-cache targets),
  # the heavy lifting is done by the .ind recipe, which writes the data (.feather)
  # file, posts the file to google drive, and writes the .feather.ind file.
  # (the local data creation and drive posting could be separated into two
  # remake targets, but let's risk having to redo the inventory for the sake of
  # keeping this remake file a touch simpler and practicing the two-target option
  # for gd_put/gd_get)

  # prepare a remake-style plan for inventorying each constituent as a separate
  # remake target in a separate remake file (tasks_1_wqp_inventory.yml)
  inventory_plan:
    command: plan_inventory(constituents=wqp_codes, folders=wqp_pull_folders)
  tasks_1_wqp_inventory.yml:
    command: create_task_makefile(
      makefile=target_name,
      task_plan=inventory_plan,
      include='1_wqpdata.yml',
      packages=I(c('dplyr', 'dataRetrieval', 'feather', 'scipiper')),
      file_extensions=I(c('ind','feather')))

  # run the inventory pulls
  1_wqpdata/log/tasks_1_wqp_inventory.ind:
    command: loop_tasks(
      task_plan=inventory_plan, task_makefile='tasks_1_wqp_inventory.yml',
      num_tries=I(30), sleep_on_error=I(20))

  # -- create separate task plans for each constituent for data pulls --

  # prepare a remake-style plan for running each state as a separate
  # remake target in a separate remake file (tasks_1_wqp.yml)
  wqp_pull_plan:
    command: plan_wqp_pull_per_constituent(
      constituents=wqp_codes,
      folders = wqp_pull_folders,
      folders_item=I('wqp_pull_folders'))
  tasks_1_wqp.yml:
    command: create_task_makefile(
      makefile=target_name,
      task_plan=wqp_pull_plan,
      include='1_wqpdata.yml',
      packages=I(c('dplyr', 'dataRetrieval', 'feather', 'scipiper')),
      file_extensions=I(c('ind','feather')),
      ind_complete=TRUE)

  # -- run the data pulls --

  1_wqpdata/log/tasks_1_wqp.ind:
    command: loop_tasks(
      task_plan=wqp_pull_plan, task_makefile='tasks_1_wqp.yml',
      task_names=I(c('pH','DO','conductivity')),
      num_tries=I(30), sleep_on_error=I(20))
